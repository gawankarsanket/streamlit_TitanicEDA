# -*- coding: utf-8 -*-
"""EDA_Titanicwith external File input.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IvkfCUbsqXeQwshuP8BIX7WMbtHCZUMQ
"""

import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder # OHE for input feature
from sklearn.preprocessing import StandardScaler # Normalise data
from sklearn.impute import SimpleImputer    # Handle missing value
from sklearn.compose import ColumnTransformer  #To apply  separate transformers for numerical and categorical data
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier



# Function to load and preprocess the datasets
def load_and_preprocess_datasets(train_file, test_file):
    # Load training dataset
    df_train = pd.read_csv(train_file)

    # Load test dataset
    df_test = pd.read_csv(test_file)

    # Perform any additional preprocessing steps if needed

    return df_train, df_test

# Streamlit App
def main():
    st.title("EDA and Prediction on Titanic Dataset")
    st.header("Dataset Uploader and Preprocessor")

    # Upload training dataset
    st.sidebar.header("Upload Training Dataset")
    train_file = st.sidebar.file_uploader("Choose a CSV file", type=["csv"],key="train_dataset")

    # Upload test dataset
    st.sidebar.header("Upload Test Dataset")
    test_file = st.sidebar.file_uploader("Choose a CSV file", type=["csv"],key="test_dataset")

    # Check if datasets are uploaded
    if train_file is not None and test_file is not None:
        # Load and preprocess datasets
        df_train, df_test = load_and_preprocess_datasets(train_file, test_file)

        # Display information about the datasets
        st.subheader("Training Dataset:")
        st.write(df_train.head())

        st.subheader("Test Dataset:")
        st.write(df_test.head())

        ######  EDA   #####

        #Number of people survived male Female
        st.subheader("Male-Female Survived and death count")
        st.write(df_train.groupby(['Sex','Survived'])['PassengerId'].count())

        #Number of people survived at each class
        st.subheader("Class wide Survived and death count")
        st.write(df_train.groupby(['Pclass','Survived'])['PassengerId'].count())

        #Survival Ratio in Male passenger
        Total_male_passegers=df_train[df_train['Sex'] == 'male']['PassengerId'].count()
        survived_male_passengers=df_train[(df_train['Sex']=='male')&(df_train['Survived']==1)]['PassengerId'].count()
        Survival_ratio_male=survived_male_passengers/Total_male_passegers

        #Visualisation
        # Your data
        labels1 = ['Survived', 'Not Survived']
        sizes1 = [Survival_ratio_male * 100, (1 - Survival_ratio_male) * 100]
        colors1 = ['green', 'red']

        # Plotting
        fig1,ax1 = plt.subplots()
        ax1.pie(sizes1, labels=labels1, colors=colors1, autopct='%1.1f%%', startangle=90)
        ax1.axis('equal')  # Equal aspect ratio ensures that the pie chart is circular.
        ax1.set_title('Survival Ratio for Male Passengers')
        st.pyplot(fig1)


        #Survival ration in Female passenger
        Total_female_passegers=df_train[df_train['Sex']=='female']['PassengerId'].count()
        survived_female_passengers=df_train[(df_train['Sex']=='female') & (df_train['Survived']==1)]['PassengerId'].count()
        Survival_ratio_female=survived_female_passengers/Total_female_passegers


        #Visualisation
        # Your data
        labels2 = ['Survived', 'Not Survived']
        sizes2 = [Survival_ratio_female * 100, (1 - Survival_ratio_female) * 100]
        colors2 = ['green', 'red']

        # Plotting
        fig2,ax2 = plt.subplots()
        ax2.pie(sizes2, labels=labels2, colors=colors2, autopct='%1.1f%%', startangle=90)
        ax2.axis('equal')  # Equal aspect ratio ensures that the pie chart is circular.
        ax2.set_title('Survival Ratio for Female Passengers')
        st.pyplot(fig2)


        #Survival ration in Kids passenger
        Total_kid_passegers=df_train[df_train['Age']<=18]['PassengerId'].count()
        survived_kid_passengers=df_train[(df_train['Age']<=18) & (df_train['Survived']==1)]['PassengerId'].count()
        Survival_ratio_kid=survived_kid_passengers/Total_kid_passegers

        #Visualisation
        # Your data
        labels3 = ['Survived', 'Not Survived']
        sizes3 = [Survival_ratio_kid * 100, (1 - Survival_ratio_kid) * 100]
        colors3 = ['green', 'red']

        # Plotting
        fig3,ax3 = plt.subplots()
        ax3.pie(sizes3, labels=labels1, colors=colors3, autopct='%1.1f%%', startangle=90)
        ax3.axis('equal')  # Equal aspect ratio ensures that the pie chart is circular.
        ax3.set_title('Survival Ratio for Kids Passengers')
        st.pyplot(fig3)


        #Total Survival Ratio
        total_passengers=df_train['PassengerId'].count()
        total_survived_passengers=df_train[df_train['Survived']==1]['PassengerId'].count()
        Survival_ratio=total_survived_passengers/total_passengers

        # Your data
        labels4 = ['Survived', 'Not Survived']
        sizes4 = [Survival_ratio* 100, (1 - Survival_ratio) * 100]
        colors4 = ['green', 'red']

        # Plotting
        fig4,ax4 = plt.subplots()
        ax4.pie(sizes4, labels=labels4, colors=colors4, autopct='%1.1f%%', startangle=90)
        ax4.axis('equal')  # Equal aspect ratio ensures that the pie chart is circular.
        ax4.set_title('Survival Ratio of all Passengers')
        st.pyplot(fig4)


        #Survival and death count of passeger wrt their source station
        st.subheader("Survival count wrt thier source station")
        st.write(df_train.groupby(['Embarked','Survived'])['PassengerId'].count())


        ### Prediction using ML algorithm   ###

        st.subheader("NUll values in Columns")
        st.write(df_train.isnull().sum())  #There is null at age columns and Cabin. for Age we can fillna using avg value.
        #Cabin 687 empty and also we can note remove 687 rows out of 891.
        #SO better drop that column only


        #Before we fiil that, lets see if there is an outlier in Age  colunmns
        sns.boxplot(df_train['Age'])
        # We can see there are outlier in age col but these are valid age values.
        #Keep them as it is and replace null with median

        df_train['Age'].fillna(df_train['Age'].median(),inplace=True)
        df_train.isnull().sum()

        df_test['Age'].fillna(df_test['Age'].median(),inplace=True)
        df_test.isnull().sum()

        # Lets take relavant columns only.  NAME, CABIN, FARE are not relavant to survaival
        x_train=df_train[['PassengerId','Pclass','Sex','Age','SibSp','Parch']]
        y_train=df_train['Survived']

        x_test=df_test[['PassengerId','Pclass','Sex','Age','SibSp','Parch']]

        # Define separate list of columns as numeric and object

        lst_num=[]
        lst_cat=[]

        for i in range(len(x_train.columns)):
            if x_train.dtypes[i]=='object':
                lst_cat.append(x_train.columns[i])

            else:
                lst_num.append(x_train.columns[i])


        ## Feature Engg Automation
        #Create pipeline to process Cat data and Num data.

        num_pipeline=Pipeline(
            steps=[
                #('Imputer',SimpleImputer(strategy='mean')), #Handling missing values
                #Not needed as we have already handled mising values( age column)

                ('Scaling',StandardScaler()) ##Feature Scaling/Normalisation
                ]
           )

        cat_pipeline=Pipeline(
            steps=[
                # ('Imputer',SimpleImputer(strategy='most_frequent')), #Handling missing value
                ##Not needed as we have already handled mising values( age column)
                ('OHE',OneHotEncoder())  # Categorical to numeric
               ]
           )


        preprocessor=ColumnTransformer([
            ('numpipeline',num_pipeline,lst_num),
            ('catpipeline',cat_pipeline,lst_cat)
           ])


        #Apply scaling , Imputation, encoding to x data as defined in pipeline >> Preprocessor
        #as defined in


        x_train=preprocessor.fit_transform(x_train)
        x_test=preprocessor.transform(x_test)



        ## Model Training Automation
        models={
            'Random Forest':RandomForestClassifier(),
            'Decision Tree':DecisionTreeClassifier()
            }


        from sklearn.metrics import accuracy_score
        def evaluate_model (x_train,y_train,x_test,models):
        
          report={}
          for i in range(len(models)):
            try:
                model=list(models.values())[i]
                model.fit(x_train, y_train)
                preds=model.predict(x_test)
                report[list(models.keys())[i]]=preds
            except Exception as e:
                st.write(f"Error in {model}: {e}")
            
          return report

        #CALL for FUNCTION
        Report=evaluate_model(x_train,y_train,x_test,models)
        #st.write(Report)

        #Add these report as column in Test dataframe

        df_test_predicted=pd.concat([df_test, pd.DataFrame(Report)],axis=1)
        df_test_predicted.rename(columns={'Random Forest': 'Survival_Pred_by_RF', 'Decision Tree': 'Survival_Pred_by_DT'}, inplace=True)

        st.subheader("PREDICTION ARE :")
        st.write(df_test_predicted)
        st.write(df_test_predicted.columns)

        # Perform further analysis or model training if needed

if __name__ == "__main__":
    main()

